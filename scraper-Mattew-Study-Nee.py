import requests
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin
import time
import random

BASE_URL = "http://lightinnj.org/%E5%80%AA%E6%9F%9D%E8%81%B2%E6%96%87%E9%9B%86/%E5%80%AA%E6%9F%9D%E8%81%B2%E6%96%87%E9%9B%86%E7%AC%AC%E4%B8%80%E8%BE%91/15%E9%A9%AC%E5%A4%AA%E7%A6%8F%E9%9F%B3%E6%9F%A5%E7%BB%8F%E8%AE%B0%E5%BD%95/%E9%A9%AC%E5%A4%AA%E7%A6%8F%E9%9F%B3%E6%9F%A5%E7%BB%8F%E8%AE%B0%E5%BD%95%E7%9B%AE%E5%BD%95.htm"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
}

def fetch_html(url: str, encoding: str = "gb18030") -> str:
    """Fetch HTML with proper encoding and polite delay"""
    try:
        r = requests.get(url, headers=HEADERS, timeout=20)
        r.encoding = encoding
        time.sleep(random.uniform(0.5, 1.2))  # Polite crawling
        return r.text
    except Exception as e:
        print(f"âŒ Failed to fetch {url}: {e}")
        return ""

def extract_chapter_links(html: str, base_url: str) -> list[tuple[str, str]]:
    """Extract chapter links from the table of contents
    Returns: list of (chapter_title, chapter_url)
    """
    soup = BeautifulSoup(html, "html.parser")
    chapters = []

    # Find all links that contain "ç¬¬" and "ç« "
    for a in soup.find_all("a", href=True):
        text = a.get_text(strip=True)
        if "ç¬¬" in text and "ç« " in text:
            href = a.get("href")
            if href and not href.startswith("#"):
                full_url = urljoin(base_url, href)
                chapters.append((text, full_url))

    return chapters

def extract_content(html: str) -> str:
    """Extract the third <p> tag content from the nested table structure
    Structure: html>body>div>table>tbody>tr>td>table>tbody>tr>td>p[2] (third p)
    """
    if not html:
        return ""

    soup = BeautifulSoup(html, "html.parser")

    # Navigate through the nested structure
    body = soup.find("body")
    if not body:
        return ""

    # Find the main content div/table structure
    # Try to find all <p> tags and get the third one with substantial content
    all_ps = soup.find_all("p")

    # Filter out empty paragraphs and get the ones with actual content
    content_ps = [p for p in all_ps if p.get_text(strip=True)]

    if len(content_ps) < 3:
        # If less than 3 paragraphs, try to get the longest one
        if content_ps:
            target_p = max(content_ps, key=lambda p: len(p.get_text(strip=True)))
        else:
            return ""
    else:
        # Get the third paragraph with content
        target_p = content_ps[2]

    # Convert to markdown-friendly text
    # Replace <br> with newlines
    for br in target_p.find_all("br"):
        br.replace_with("\n")

    text = target_p.get_text(separator="\n", strip=False)

    # Clean up excessive whitespace and newlines
    lines = [line.strip() for line in text.split("\n")]
    lines = [line for line in lines if line]  # Remove empty lines

    return "\n\n".join(lines)

def build_matthew_study_markdown() -> str:
    """Main function to scrape all chapters and build markdown"""
    print("ğŸ“– å¼€å§‹æŠ“å–é©¬å¤ªç¦éŸ³æŸ¥ç»è®°å½•...")

    # Fetch table of contents
    print(f"ğŸ“¥ Fetching table of contents: {BASE_URL}")
    toc_html = fetch_html(BASE_URL)
    if not toc_html:
        print("âŒ Failed to fetch table of contents")
        return ""

    # Extract chapter links
    chapters = extract_chapter_links(toc_html, BASE_URL)
    print(f"âœ… Found {len(chapters)} chapters")

    # Build markdown
    md_lines = ["# é©¬å¤ªç¦éŸ³æŸ¥ç»è®°å½•\n"]

    for idx, (title, url) in enumerate(chapters, start=1):
        print(f"ğŸ“¥ Fetching chapter {idx}/{len(chapters)}: {title}")

        # Fetch chapter content
        chapter_html = fetch_html(url)
        content = extract_content(chapter_html)

        # Add to markdown
        md_lines.append(f"## {title}\n")
        if content:
            md_lines.append(content + "\n")
        else:
            md_lines.append("_ï¼ˆæœ¬ç« æœªæ£€æµ‹åˆ°å†…å®¹ï¼‰_\n")
            print("â—ï¸ empty content detected")

        md_lines.append("")  # Blank line between chapters

    return "\n".join(md_lines).strip() + "\n"

if __name__ == "__main__":
    markdown_content = build_matthew_study_markdown()

    if markdown_content:
        output_file = "é©¬å¤ªç¦éŸ³æŸ¥ç»è®°å½•.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(markdown_content)
        print(f"âœ… å·²ç”Ÿæˆï¼š{output_file}")
    else:
        print("âŒ ç”Ÿæˆå¤±è´¥")